---
title: "NYT Prediction Report"
output: html_document
---
### Exploring Data 

```{r results='hide', echo=FALSE}
options(warn = 2, error = recover)

# library load
libs = c("ggplot2", "tm", "randomForest", "caret","psych", "reshape", "ggdendro", "dplyr")
lapply(libs, library, character.only=TRUE)
submitPath = "../submit/"
# utility function load
source('../util.R')

# data load
trainData = read.csv("../data/NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
testData  = read.csv("../data/NYTimesBlogTest.csv", stringsAsFactors=FALSE)
testData$Popular = NA

## combine training set and testing until finishing data transformation
newsData = rbind(trainData, testData)
```
There are `r dim(newsData)[1]` observations in total, where `r dim(trainData)[1]` training 
observations and `r dim(testData)[1]` observations.

```{r}
popularDensity <- table(newsData$Popular)
posPopular <- round(popularDensity[2]/(popularDensity[1] + popularDensity[2]) * 100, 2)
```
Only `r posPopular`% 
of all New York Times blog articles have more than 25 comments. That means, a baseline model for predicting unpopular would be around `r 100 - posPopular`%.

The independent variables consist of 8 pieces of article data available at the time of publication, and a unique identifier:

- **NewsDesk**, the New York Times desk that produced the story (Business, Culture, Foreign, etc.)
- **SectionName**, the section the article appeared in (Opinion, Arts, Technology, etc.)
- **SubsectionName**, the subsection the article appeared in (Education, Small Business, Room for Debate, etc.)
- **Headline**, the title of the article
- **Snippet**, a small portion of the article text
- **Abstract**, a summary of the blog article, written by the New York Times
- **WordCount**, the number of words in the article
- **PubDate**, the publication date, in the format "Year-Month-Day Hour:Minute:Second"
- **UniqueID**, a unique identifier for each article

### Cleaning Data
```{r results='hide', echo=FALSE}
newsData$Summary = ifelse(nchar(cleanupText(newsData$Snippet)) > nchar(cleanupText(newsData$Abstract)),
                      cleanupText(newsData$Snippet),
                      cleanupText(newsData$Abstract))
```
##### Text of Articles
Let's take a look at **Headline**, we can observe many very common combination of words like `new york times`, `pictures of the day` etc. If we google `pictures of the day new york times`, it is easy to know
`pictures of the day` is a daily article from `Lens` category. 

```{r eval=FALSE}
newsData$Headline
```
To avoid overcounting of words like `day`, a replacement of some proper nouns to single word is necessary.
```{r}
originalText    = c("new york times", "new york city", "new york", "silicon valley", 
                    "times insider", "fashion week", "white house", 
                    "international herald tribune archive", 
                    "president obama", "hong kong", "big data", "golden globe", 
                    "word of the day", "time square", "pictures of the day",
                    "photos of the day", "daily clip report", "daily report", 
                    "6 q's about the news", "test yourself")

replacementText = c("NYT", "NYC", "NewYork", "SiliconValley", "TimesInsider",
                    "FashionWeek", "WhiteHouse", "IHT", "Obama", "HongKong",
                    "BigData", "GoldenGlobe", "WordofDay", "TimeSquare", "PicOfDay",
                    "PicOfDay", "DailyClipReport", "DailyReport", "6q", "TestYourself")
```

```{r results='hide', echo=FALSE}
newsData$Headline = phaseSub(newsData$Headline, originalText, replacementText, ignore.case=TRUE)
newsData$Summary  = phaseSub(newsData$Summary,  originalText, replacementText, ignore.case=TRUE)

rm(originalText)
rm(replacementText)

# combine Headline and Summary
newsData$Text = paste(newsData$Headline, newsData$Summary)
```

##### Word Count of Articles

```{r results='hide', echo=FALSE}
newsData$HeadlineCharCount = nchar(newsData$Headline)
newsData$SummaryCharCount  = nchar(newsData$Summary)
# Use the regular expression symbol \\W to match non-word characters, using + to indicate one or more in a row, along with gregexpr to find all matches in a string. Words are the number of word separators plus 1.
newsData$HeadlineWordCount = sapply(gregexpr("\\W+", 
                                    gsub("[[:punct:]]", "", newsData$Headline)), 
                                    length) + 1
newsData$SummaryWordCount  = sapply(gregexpr("\\W+", 
                                gsub("[[:punct:]]", "", newsData$Summary)),
                                length) + 1

newsData$Popular        = as.factor(newsData$Popular)
newsData$LogWordCount   = log(1 + newsData$WordCount)

### Distribution of LogWordCount
newsTrain = head(newsData, nrow(trainData))
newsTest  = tail(newsData, nrow(testData))

newsData$ShortHeadline = as.factor(ifelse(newsData$HeadlineCharCount <= mean(newsData$HeadlineCharCount), 1, 0))
```

The following plot shows article's popularity distribution based on logarithmic word count.
Beside training data, testing data's distribution is also plotted by gray color which is bimodal distribution.

```{r results='hide', echo=FALSE, fig.height = 4, fig.width = 7, fig.align='center'}
ggplot(newsData, aes(x=LogWordCount, fill=Popular)) + 
  geom_density(aes(y=..scaled..), alpha=0.4) +
  ggtitle("Distribution of Log WordCount") +
  xlab("Log(WordCount)") +
  theme(axis.title.y = element_blank())
```


If we conduct a two-sided t-test on the mean and a two-sided F-test on the variance:

```{r}
PopularNewsTrain   = subset(newsTrain, newsTrain$Popular==1)
UnpopularNewsTrain = subset(newsTrain, newsTrain$Popular==0)
 
t.test(PopularNewsTrain$LogWordCount, UnpopularNewsTrain$LogWordCount)
var.test(PopularNewsTrain$LogWordCount, UnpopularNewsTrain$LogWordCount)
```

This shows us there is a statistically significant difference between popular and 
unpopular articles based on the word counts. At the same time, popular article seems having 
shorter `Headline`.

```{r results='hide', echo=FALSE, fig.height = 4, fig.width = 7, fig.align='center'}
ggplot(newsData, aes(x=HeadlineCharCount, fill=Popular)) + 
  geom_density(aes(y=..scaled..), alpha=0.4) +
  ggtitle("Distribution of HeadlineCharCount") +
  xlab("# Characters in Headline") +
  theme(axis.title.y = element_blank())
```

```{r}
t.test(PopularNewsTrain$HeadlineCharCount, UnpopularNewsTrain$HeadlineCharCount)
```

```{r results='hide', echo=FALSE}
rm(PopularNewsTrain)
rm(UnpopularNewsTrain)
```

##### Publishing Hour and Day
It is unlikely that many article receiving 25 more comments in the middle of night. 
Hence, at certain times during the day, we expect the probability that a random article 
becomes popular to be larger. Similarly, the day of the week may have an impact, people
may have much more time to read articles than a working day.

```{r results='hide'}
## date feature
newsData$PubDate = strptime(newsData$PubDate, "%Y-%m-%d %H:%M:%S")
newsData$PubDay  = as.Date(newsData$PubDate)
## it is expected that different behaviours at different times of the day.publication
newsData$DayofWeek = newsData$PubDate$wday
newsData$Hour    = newsData$PubDate$hour
newsData$DayOfWeek = as.factor(weekdays(newsData$PubDate))
newsData$DayOfWeek = factor(newsData$DayOfWeek, levels=c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

## especially on holidays, people may have much more time to read and comment on blog articles
Holidays = c(as.POSIXlt("2014-09-01 00:00", format="%Y-%m-%d %H:%M"),
             as.POSIXlt("2014-10-13 00:00", format="%Y-%m-%d %H:%M"),
             as.POSIXlt("2014-10-31 00:00", format="%Y-%m-%d %H:%M"),
             as.POSIXlt("2014-11-11 00:00", format="%Y-%m-%d %H:%M"),
             as.POSIXlt("2014-11-27 00:00", format="%Y-%m-%d %H:%M"),
             as.POSIXlt("2014-12-24 00:00", format="%Y-%m-%d %H:%M"),
             as.POSIXlt("2014-12-25 00:00", format="%Y-%m-%d %H:%M"),
             as.POSIXlt("2014-12-31 00:00", format="%Y-%m-%d %H:%M"))
 
newsData$Holiday = as.factor(ifelse(newsData$PubDate$yday %in% Holidays$yday, 1, 0))
```

```{r results='hide', echo=FALSE}
newsData$PopularFactor = as.numeric(as.character(newsData$Popular))
newsData$PopularFactor[which(is.na(newsData$Popular))] = "N/A"
newsData$PopularFactor = as.factor(newsData$PopularFactor)
```

```{r echo=FALSE, fig.height = 4, fig.width = 7, fig.align = 'center'}
hourMatrix = as.matrix(table(newsData$Hour, newsData$Popular))
hourMatrix = data.frame("Unpopular" = hourMatrix[, 1], 
                        "Popular" = hourMatrix[, 2], 
                        "PopularDensity" = hourMatrix[, 2]/(hourMatrix[, 1] + hourMatrix[, 2]))
hourMatrix$Hour = 0:23
hourMatrix[order(hourMatrix$PopularDensity),]
```

It seems that publishing blog posts around `10 pm` are more easier getting popular 
according to `PopularDensity`. But, if we compare number of popular articles of 24 hours, 
It is clear to see that around `12 pm` and `3 pm`, even more blog posts receiving 25 more comments
than `10 pm`.

```{r echo=FALSE, fig.height = 4, fig.width = 7, fig.align = 'center'}
ggplot(hourMatrix, aes(x=Hour)) + 
  geom_line(aes(y=PopularDensity, color = 'PopularDensity')) +
  geom_line(aes(y=Popular/max(hourMatrix$Popular), color = 'Popular(scaled)')) +
  geom_line(aes(y=Unpopular/max(hourMatrix$Unpopular), color = 'Unpopular(scaled)')) + 
  ggtitle("Distribution of Hourly PopularDensity") +
  xlab("Hour of Day") +
  theme(axis.title.y = element_blank())
```

```{r echo=FALSE}
dayMatrix = as.matrix(table(newsData$DayOfWeek, newsData$Popular))
dayMatrix = data.frame("Unpopular" = dayMatrix[, 1], 
                        "Popular" = dayMatrix[, 2], 
                        "PopularDensity" = dayMatrix[, 2]/(dayMatrix[, 1] + dayMatrix[, 2]))
dayMatrix$Day = 1:7
dayMatrix[order(dayMatrix$PopularDensity),]
```
Similarly, day of week shows same trends as hourly results. Also, much more articles are published on
weekday than weekends.
```{r echo=FALSE, fig.height = 4, fig.width = 7, fig.align = 'center'}
ggplot(dayMatrix, aes(x=Day)) + 
  geom_line(aes(y=PopularDensity, color = 'PopularDensity')) +
  geom_line(aes(y=Popular/max(dayMatrix$Popular), color = 'Popular(scaled)')) +
  geom_line(aes(y=Unpopular/max(dayMatrix$Unpopular), color = 'Unpopular(scaled)')) + 
  ggtitle("Distribution of Day PopularDensity") +
  xlab("Day of Week") + scale_y_continuous(limits = c(0,1)) +
  theme(axis.title.y = element_blank())
```

##### Category of Articles
There are three variables categorizes blog posts, `NewsDesk` `SectionName` and `SubsectionName`.

```{r}
## missing categories
misCategory = subset(newsData, newsData$NewsDesk=="" | newsData$SectionName=="" | newsData$SubsectionName=="")
dim(misCategory)[1]
misCategory = subset(newsData, newsData$NewsDesk=="" & newsData$SectionName=="" & newsData$SubsectionName=="")
dim(misCategory)[1]
```
**6721** articles have at one category variable missing and **1626** articles have no categories at all.
After filling blank categories based on existing category variables, let's try to see the facet 
distribution of blog posts.

```{r results='hide', echo=FALSE}
rm(misCategory)

categoryMap = as.data.frame(table(newsData$NewsDesk, newsData$SectionName, newsData$SubsectionName))
names(categoryMap) = c("NewsDesk", "SectionName", "SubsectionName", "Freq")
categoryMap = subset(categoryMap, Freq > 0)
#categoryMap[order(categoryMap$SectionName),]
## fill NewsDesk by most common SectionName
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="Arts", "Culture", newsData$NewsDesk)
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="Business Day", "Business", newsData$NewsDesk)
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="Health", "Science", newsData$NewsDesk)
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="Multimedia", "", newsData$NewsDesk)
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="N.Y. / Region", "Metro", newsData$NewsDesk)
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="Open", "Technology", newsData$NewsDesk)
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="Opinion", "OpEd", newsData$NewsDesk)
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="Technology", "Business", newsData$NewsDesk)
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="Travel", "Travel", newsData$NewsDesk)
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="U.S.", "National", newsData$NewsDesk)
newsData$NewsDesk = ifelse(newsData$NewsDesk=="" & newsData$SectionName=="World", "Foreign", newsData$NewsDesk)

## fix Crosswords
idx = which(newsData$SectionName=="Crosswords/Games")
newsData$NewsDesk[idx]       = "Styles"
newsData$SectionName[idx]    = "Puzzles"
newsData$SubsectionName[idx] = ""
## fix U.S.
idx = which(newsData$NewsDesk=="Styles" & newsData$SectionName=="U.S.")
newsData$NewsDesk[idx]       = "Styles"
newsData$SectionName[idx]    = "Style"
newsData$SubsectionName[idx] = ""

#categoryMap[order(categoryMap$NewsDesk),]
## fill SectionName by most common NewsDesk
newsData$SectionName = ifelse(newsData$SectionName=="" & newsData$NewsDesk=="Culture", "Arts", newsData$SectionName)
newsData$SectionName = ifelse(newsData$SectionName=="" & newsData$NewsDesk=="Foreign", "World", newsData$SectionName)
newsData$SectionName = ifelse(newsData$SectionName=="" & newsData$NewsDesk=="National", "U.S.", newsData$SectionName)
newsData$SectionName = ifelse(newsData$SectionName=="" & newsData$NewsDesk=="OpEd", "Opinion", newsData$SectionName)
newsData$SectionName = ifelse(newsData$SectionName=="" & newsData$NewsDesk=="Science", "Science", newsData$SectionName)
newsData$SectionName = ifelse(newsData$SectionName=="" & newsData$NewsDesk=="Sports", "Sports", newsData$SectionName)
newsData$SectionName = ifelse(newsData$SectionName=="" & newsData$NewsDesk=="Styles", "Style", newsData$SectionName)
newsData$SectionName = ifelse(newsData$SectionName=="" & newsData$NewsDesk=="TStyle", "Magazine", newsData$SectionName)

## fill all empty ( NewsDesk, SectionName, SubsectionName ) cases
idx = which(newsData$NewsDesk == "" & newsData$SectionName == "" & newsData$SubsectionName == "" &
              grepl("^(first draft|lunchtime laughs|politics helpline|today in politics|verbatim)",
                    newsData$Headline, ignore.case=TRUE))
newsData$NewsDesk[idx]       = "National"
newsData$SectionName[idx]    = "U.S."
newsData$SubsectionName[idx] = "Politics"

## fill empty SectionName based on political terms
idx = which(newsData$SectionName=="" &
              grepl(paste0("white house|democrat|republican|tea party|",
                           "obama|biden|boehner|kerry|capitol|senat|",
                           "sen\\.|congress|president|washington|politic|",
                           "rubio|palin|clinton|bush|limbaugh|rand paul|",
                           "christie|mccain|election|poll|cruz|constitution|",
                           "amendment|federal|partisan|yellen|govern|",
                           "gov\\.|legislat|supreme court|campaign|",
                           "primary|primaries|justice|jury"),
                    newsData$Text, ignore.case=TRUE))
newsData$NewsDesk[idx]       = "National"
newsData$SectionName[idx]    = "U.S."
newsData$SubsectionName[idx] = "Politics"

newsData$NewsDesk[which(newsData$NewsDesk=="")]             = "Missing"
newsData$SectionName[which(newsData$SectionName=="")]       = "Missing"
newsData$SubsectionName[which(newsData$SubsectionName=="")] = "Missing"

newsData$SectionNameFactor <- as.factor(newsData$SectionName)
newsData$NewsDeskFactor <- as.factor(newsData$NewsDesk)
newsData$SubsectionNameFactor <- as.factor(newsData$SubsectionName)

rm(idx)

## PubDay vs popular
DailyArticles        = as.data.frame(table(newsData$PubDay))
names(DailyArticles) = c("PubDay", "NumDailyArticles")
DailyArticles$PubDay = as.Date(as.character(DailyArticles$PubDay), format="%Y-%m-%d")
newsData             = merge(newsData, DailyArticles, by = "PubDay", all.x=TRUE)

## PubDay per section vs popular
DailySectionArticles        = as.data.frame(table(newsData$PubDay, newsData$SectionName))
names(DailySectionArticles) = c("PubDay", "SectionName", "NumDailySectionArticles")
DailySectionArticles$PubDay = as.Date(as.character(DailySectionArticles$PubDay), format="%Y-%m-%d")
newsData                    = merge(newsData, DailySectionArticles, all.x=TRUE)

## hourly published vs popular
HourlyArticles        = as.data.frame(table(newsData$PubDay, newsData$Hour))
names(HourlyArticles) = c("PubDay", "Hour", "NumHourlyArticles")
HourlyArticles$PubDay = as.Date(as.character(HourlyArticles$PubDay), format="%Y-%m-%d")
newsData              = merge(newsData, HourlyArticles, all.x=TRUE)

```

```{r echo=FALSE, fig.height = 7, fig.width = 10, fig.align = 'center'}
newsTrain = head(newsData, nrow(trainData))
newsTest  = tail(newsData, nrow(testData))

#ggplot(newsTrain, aes(x=NumDailyArticles, fill=PopularFactor)) + 
#  geom_density(aes(y=..scaled..), alpha=0.4) +
#  ggtitle("Distribution of NumDailyArticles") +
#  xlab("# Daily Articles Published") +
#  scale_fill_discrete(name="Popular") +
#  theme(axis.title.y = element_blank())

ggplot(newsTrain, aes(x=NumDailySectionArticles, fill=PopularFactor)) +
  geom_density(aes(y=..scaled..), alpha=0.4) +
  ggtitle("Distribution of NumDailySectionArticles") +
  xlab("# Daily Articles Published facet by Section") +
  scale_fill_discrete(name="Popular") +
  theme(axis.title.y = element_blank()) + 
  facet_wrap( ~ SectionName, ncol=3)
```

Although it is hard to see what's going on, a clear difference between popular and unpopular 
articles is in the section Magazine, where around 15 articles posted per day is more 
indicative of popular articles than unpopular ones. Beyond 20 posts per day the roles are 
clearly reversed. Hourly distribution of the following plot also shows no clear indication
of popularity.

```{r echo=FALSE, fig.height = 4, fig.width = 10, fig.align = 'center'}
ggplot(newsTrain, aes(x=NumHourlyArticles, fill=PopularFactor)) +
  geom_density(aes(y=..scaled..), alpha=0.4) +
  ggtitle("Distribution of NumHourlyArticles") +
  xlab("# Hourly Articles Published") +
  scale_fill_discrete(name="Popular") +
  theme(axis.title.y = element_blank()) + 
  facet_wrap( ~ Hour, ncol=8)
```

##### Contents Features of Articles
Until now, we preprocessed date features, word counts and categories of article. Almost all features
from original data frame but the contents of blog post.


```{r}
stopWords = c(stopwords("SMART"))
CorpusText = Corpus(VectorSource(newsData$Headline))
CorpusText = tm_map(CorpusText, tolower)
CorpusText = tm_map(CorpusText, PlainTextDocument)
CorpusText = tm_map(CorpusText, removePunctuation)
CorpusText = tm_map(CorpusText, removeWords, stopWords)
CorpusText = tm_map(CorpusText, stemDocument, language="english")

tdmText = TermDocumentMatrix(CorpusText)
# around 20 words
sparseText = removeSparseTerms(tdmText, 0.989)
sparseText = as.data.frame(as.matrix(sparseText))
colnames(sparseText) = make.names(colnames(sparseText))

dtmText = DocumentTermMatrix(CorpusText)
freqTerms = findFreqTerms(dtmText, lowfreq=10)
termFreq  = colSums(as.matrix(dtmText))
termFreq  = subset(termFreq, termFreq>=100)
df        = data.frame(term=names(termFreq), freq=termFreq)

## Bag of Words
newsDataBoW = newsData
newsDataBoW$PubDate = NULL

tSparseText = t(sparseText)
colnames(tSparseText) = make.names(paste('c',colnames(tSparseText),sep="_"))
newsDataBoW[, colnames(tSparseText)] = tSparseText
```

```{r fig.height = 7, fig.width = 7, fig.align = 'center', echo=FALSE}
ggplot(df, aes(x=reorder(term, freq, max), y=freq)) +
  geom_bar(stat="identity") +
  ggtitle("Most Common Terms in the Headline") +
  xlab("Terms") +
  ylab("Frequency") +
  coord_flip()
```

### Modeling Data
```{r results='hide', echo=FALSE}
newsData$PubDate <- NULL
newsTrain = head(newsData, nrow(trainData))
newsTest  = tail(newsData, nrow(testData))
```

##### Logistic Regression without contents feature
```{r}
# MODEL--0
# LR_0
# finding importance of features
removedColumns = c("SectionName", "NewsDesk", "SubsectionName", "Headline", "Snippet", "Abstract", "Summary", "UniqueID", "Text", "PopularFactor")
LR_0 = glm(Popular ~ ., data=newsTrain[,!colnames(newsTrain) %in% removedColumns], family=binomial)

calcAUClr(LR_0, newsTrain$Popular)
# training result: 0.9315814
LR_0_Pred = predict(LR_0, newdata=newsTest[, !colnames(newsTrain) %in% removedColumns], type="response")
generateSubmission(LR_0_Pred, submitPath)
# testing  result: 0.89088

summary(LR_0)
```
By using `summary(Model)`, we can select the following variables based on importance of variables.
```{r}
selectedColumns = c("Popular", 
                    "PubDay", 
                    "Hour", 
                    "LogWordCount", 
                    "SummaryCharCount", 
                    "HeadlineWordCount", 
                    "WordCount", 
                    "SummaryCharCount", 
                    "HeadlineCharCount", 
                    "SectionNameFactor", 
                    "SubsectionNameFactor", 
                    "NumDailySectionArticles", 
                    "NumHourlyArticles")
```
###### MODEL-1 (logistic regression)
```{r results='hide'}
LR_1 = glm(Popular ~ ., data=newsTrain[, colnames(newsTrain) %in% selectedColumns], family=binomial)

calcAUClr(LR_1, newsTrain$Popular)
# training result: 0.9297705

LR_1_Pred = predict(LR_1, newdata=newsTest[, colnames(newsTrain) %in% selectedColumns], type="response")
generateSubmission(LR_1_Pred, submitPath)
# testing  result: 0.89212 
```

###### MODEL-2 (random forest) 
```{r results='hide'}
RF_2 = randomForest(Popular ~ ., data=newsTrain[, colnames(newsTrain) %in% selectedColumns], nodesize=1, ntree=2500, importance=TRUE)

trainPartition = createDataPartition(y=newsTrain$Popular, p=0.5, list=FALSE)
tuneTrain      = newsTrain[trainPartition, colnames(newsTrain) %in% selectedColumns]
RF_2.tuned  = train(Popular ~ ., data=tuneTrain, method="rf", trControl=trainControl(method="cv", number=5))

calcAUC(RF_2, newsTrain$Popular)
# training result: 0.9387521

RF_2_Pred = predict(RF_2, newdata=newsTest, type="prob")[,2]
generateSubmission(RF_2_Pred, submitPath)
# testing  result: 0.89470
```

###### MODEL-3 (logistic regression on Bag of Words)
```{r results='hide', echo=FALSE}
newsDataBoW$PubDate <- NULL
newsTrainBoW = head(newsDataBoW, nrow(trainData))
newsTestBoW  = tail(newsDataBoW, nrow(testData))
```

```{r results='hide'} 
LR_3 = glm(Popular ~ ., data=newsTrainBoW[,!colnames(newsTrainBoW) %in% removedColumns], family=binomial)

calcAUClr(LR_3, newsTrainBoW$Popular)
# training result: 0.9389116

LR_3_Pred = predict(LR_3, newdata=newsTestBoW, type="response")
generateSubmission(LR_3_Pred, submitPath)
# testing  result: 0.90176
```

###### MODEL-4 (random forest on Bag of Words)
```{r results='hide'}
RF_4 = randomForest(Popular ~ ., data=newsTrainBoW[,!colnames(newsTrainBoW) %in% removedColumns], nodesize=5, ntree=2500, importance=TRUE)

trainPartition = createDataPartition(y=newsTrainBoW$Popular, p=0.5, list=FALSE)
tuneTrain      = newsTrainBoW[trainPartition,!colnames(newsTrainBoW) %in% removedColumns]
RF_4.tuned  = train(Popular ~ ., data=tuneTrain, method="rf", trControl=trainControl(method="cv", number=5))

calcAUC(RF_4, newsTrainBoW$Popular)
# training result: 0.9399500

RF_4_Pred = predict(RF_4, newdata=newsTestBoW, type="prob")[,2]
generateSubmission(RF_4_Pred, submitPath)
# testing  result: 0.90048

```

##### Supervised + Unsupervised 
```{r results='hide'}
matrixSparseText  = as.matrix(sparseText)
matrixSparseText.distMatrix = dist(scale(matrixSparseText))
matrixSparseText.clusters   = hclust(matrixSparseText.distMatrix, method="ward.D2")

dText = as.dendrogram(matrixSparseText.clusters)
dTextData <- dendro_data(dText, type = "rectangle")

ggplot(segment(dTextData)) + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) + coord_flip() 

kText   = 25
mText   = t(sparseText)
KMCText = kmeans(mText, kText)
 
for (i in 1:kText) {
  cat(paste("cluster", i, ": ", sep=" "))
  s = sort(KMCText$centers[i, ], decreasing=TRUE)
  cat(names(s)[1:15], sep=", ", "\n")
}

newsData$TextCluster     = as.factor(KMCText$cluster)
newsData$PubDate = NULL
newsTrain = head(newsData, nrow(trainData))
newsTest  = tail(newsData, nrow(testData))
```
###### MODEL-5 (random forest on text cluster)
```{r results='hide'}
selectedColumns = cbind(selectedColumns, c("TextCluster"))
RF_5 = randomForest(Popular ~ ., data=newsTrain[, colnames(newsTrain) %in% selectedColumns], nodesize=5, ntree=2500, importance=TRUE)

trainPartition = createDataPartition(y=newsTrain$Popular, p=0.5, list=FALSE)
tuneTrain      = newsTrain[trainPartition, colnames(newsTrain) %in% selectedColumns]
RF_5.tuned  = train(Popular ~ ., data=tuneTrain, method="rf", trControl=trainControl(method="cv", number=5))

calcAUC(RF_5, newsTrain$Popular)
# training result: 0.9399861 

RF_5_Pred = predict(RF_5, newdata=newsTest, type="prob")[,2]
generateSubmission(RF_5_Pred, submitPath)
# testing  result: 0.89854 
```

##### Features of negative example
```{r results='hide'}
newsData$QAboutNews = as.factor(ifelse(grepl("6q", newsData$Text) == TRUE, 1, 0))
newsData$DailyClip = as.factor(ifelse(grepl("DailyClipReport", newsData$Text) == TRUE, 1, 0))
newsData$DailyReport = as.factor(ifelse(grepl("DailyReport", newsData$Text) == TRUE, 1, 0))
newsData$PicOfDay = as.factor(ifelse(grepl("PicOfDay", newsData$Text) == TRUE, 1, 0))
newsData$TestYourself = as.factor(ifelse(grepl("TestYourself", newsData$Text) == TRUE, 1, 0))
newsData$WordofDay = as.factor(ifelse(grepl("WordofDay", newsData$Text) == TRUE, 1, 0))
newsData$Recap = as.factor(ifelse(grepl("recap", newsData$Text) == TRUE, 1, 0))

newsTrain = head(newsData, nrow(trainData))
newsTest  = tail(newsData, nrow(testData))
```

###### MODEL-6 (logistic regression with negative example features)
```{r eval=FALSE}
selectedColumns = cbind(selectedColumns, c("QAboutNews", "DailyClip", "DailyReport", "PicOfDay", "TestYourself", "WordofDay", "Recap"))

LR_6 = glm(Popular ~ ., data=newsTrain[, colnames(newsTrain) %in% selectedColumns], family=binomial)
calcAUClr(LR_6, newsTrain$Popular)

LR_6_Pred = predict(LR_6, newdata=newsTest[, colnames(newsTest) %in% selectedColumns], type="response")
generateSubmission(LR_6_Pred, submitPath)
```

###### MODEL-7 (random forest with negative example features)
```{r results='hide'}
RF_7 = randomForest(Popular ~ ., data=newsTrain[, colnames(newsTrain) %in% selectedColumns], nodesize=5, ntree=2500, importance=TRUE)

trainPartition = createDataPartition(y=newsTrain$Popular, p=0.5, list=FALSE)
tuneTrain      = newsTrain[trainPartition, colnames(newsTrain) %in% selectedColumns]

RF_7.tuned  = train(Popular ~ ., data=tuneTrain, method="rf", trControl=trainControl(method="cv", number=5))

calcAUC(RF_7, newsTrain$Popular)
# training result: 0.9390981

RF_7_Pred = predict(RF_7, newdata=newsTest, type="prob")[,2]
generateSubmission(RF_7_Pred, submitPath)
# testing  result: 0.89867
```

##### Topic Words
Add some topic features according [Topic from Google].
```{r results='hide'}
questionWords <- c("\\?","^why","should","^when","can","^if","^is")
religionWords <- c("secular","humanist","humanism","secularist","god","religion","atheist","atheism","islam","islamic","islamist",
              "islamists","church","atheists","jesus","christ","christian","catholic","pope","imam", "\\<isis\\>","muslim","gay","marriage","israel","jewish","extremist", "fundamentalism","terror","terrorist","terrorism")
techWords <- c("apple","\\<ios\\>","ipod", "ipad","iphone")
healthWords <- c("cancer","weight","fat","heart","disease","brain","sex","sexual","love","hate","doctor","doctors","medical","medicine","hospital","hospitals")
sciWords <- c("climate","warming","global","science","scientists")
busWords <- c("jobs","employment","work","working","economy")
poliHiWords <- c("republican", "conservative")

newsData$Question <- as.factor(ifelse(grepl(paste(questionWords, collapse="|"), newsData$Headline)==TRUE,1,0))
newsData$religionWords <- ifelse(grepl(paste(religionWords, collapse="|"), newsData$Headline)==TRUE,1,0)
newsData$tech <- ifelse(grepl(paste(techWords, collapse="|"), newsData$Headline)==TRUE,1,0)
newsData$health <- ifelse(grepl(paste(healthWords, collapse="|"), newsData$Headline)==TRUE,1,0)
newsData$sci <- ifelse(grepl(paste(sciWords, collapse="|"), newsData$Headline)==TRUE,1,0)
newsData$business <- ifelse(grepl(paste(busWords, collapse="|"), newsData$Headline)==TRUE,1,0)
newsData$poliHi <- ifelse(grepl(paste(poliHiWords, collapse="|"), newsData$Headline)==TRUE,1,0)
newsData$noComment <- ifelse(grepl("no comment necessary", newsData$Headline),1,0)
newsData$comments <- ifelse(grepl("open for comments", newsData$Headline),1,0)
```

```{r results='hide', echo=FALSE}
newsData$Headline  <- NULL
newsData$Snippet <- NULL
newsData$Abstract <- NULL
newsData$Summary <- NULL
newsData$Text <- NULL
newsData$PopularFactor <- NULL
newsData$SectionName <- NULL
newsData$NewsDesk <- NULL
newsData$SubsectionName <- NULL

newsTrain = head(newsData, nrow(trainData))
newsTest  = tail(newsData, nrow(testData))
```

####### MODEL-8 (logistic regression with topic words)
```{r results='hide'}
LR_8 = glm(Popular ~ . - UniqueID, data=newsTrain, family=binomial)
calcAUClr(LR_8, newsTrain$Popular)
# training result: 0.9412723 

LR_8_Pred = predict(LR_8, newdata=newsTest, type="response")
generateSubmission(LR_8_Pred, submitPath)
# testing  result: 0.89517
```

###### MODEL-9 (random forest with topic words)
```{r results='hide'}
RF_9 = randomForest(Popular ~ . - UniqueID, data=newsTrain, nodesize=5, ntree=2500, importance=TRUE)
trainPartition = createDataPartition(y=newsTrain$Popular, p=0.5, list=FALSE)
tuneTrain      = newsTrain[trainPartition, ]
RF_9.tuned  = train(Popular ~ . - UniqueID, data=tuneTrain, method="rf", trControl=trainControl(method="cv", number=5))

calcAUC(RF_9, newsTrain$Popular)
# training result: 0.9418562

RF_9_Pred = predict(RF_9, newdata=newsTest, type="prob")[,2]
generateSubmission(RF_9_Pred, submitPath)
# testing  result: 0.89962
```

##### Model Evaluation
| Model Name | Train ROC | Test ROC |    DESC     |
| ---------- | --------- | -------- | ----------- |
|  LR_0      | 0.9315814 | 0.89088 | logistic regression  |
|  LR_1      | 0.9297705 | 0.89212 | logistic regression on selected variables |
|  LR_3      | 0.9389116 | **0.90176** | logistic regression with bag of words |
|  LR_6      | 0.9367199 | 0.90108 | logistic regression with negative features |
|  LR_8      | 0.9411255 | 0.89801 | logistic regression with topic words  |  
|  RF_2      | 0.9388249 | 0.89368 | random forest |
|  RF_4      | 0.9399030 | 0.89965 | random forest with bag of words |
|  RF_5      | 0.9387642 | 0.89865 | random forest on text cluster |
|  RF_7      | 0.9389404 | 0.89732 | random forest with negative features |
|  RF_9      | 0.941084  | 0.89887 | random forest with topic words | 

##### Ensemble
```{r eval=FALSE}
EN_1 = (2*RF_9_Pred + 3*RF_4_Pred + 1*LR_3_Pred)/6
EN_2 = (2*RF_4_Pred + 3*LR_3_Pred + 1*LR_3_Pred)/6
EN_3 = (2*LR_1_Pred + 3*RF_2_Pred + 1*LR_8_Pred)/6
EN_4 = (2*LR_6_Pred + 3*RF_5_Pred + 1*RF_7_Pred)/6
EN_5 = (2*RF_5_Pred + 3*RF_4_Pred + 1*LR_3_Pred)/6
EN_6 = (1*RF_4_Pred + 2*LR_3_Pred + 1*LR_3_Pred)/4

generateSubmission(EN_1, submitPath)
generateSubmission(EN_2, submitPath)
generateSubmission(EN_3, submitPath)
generateSubmission(EN_4, submitPath)
generateSubmission(EN_5, submitPath)
generateSubmission(EN_6, submitPath)

```
| Model Name | Test ROC |
| ---------- | -------- |
|  EN_1      | 0.90238  | 
|  EN_2      | **0.90550**  |
|  EN_3      | 0.89973  |
|  EN_4      | 0.90361  |
|  EN_5      | 0.90271  |
|  EN_6      | 0.90529  |

[Topic from Google]:http://www.google.com/trends/topcharts#vm=cat&geo=US&date=2014&cid=travel_and_leisure
